{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "def init_process(rank: int, size: int, fn: Callable[[int, int], None], backend=\"gloo\"):\n",
    "    \"\"\"Initialize the distributed environment.\"\"\"\n",
    "    os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_reduce(rank: int, size: int):\n",
    "    # create a group with all processors\n",
    "    group = dist.new_group(list(range(size)))\n",
    "    tensor = torch.ones(1)\n",
    "    # sending all tensors to rank 0 and sum them\n",
    "    dist.reduce(tensor, dst=0, op=dist.ReduceOp.SUM, group=group)\n",
    "    # can be dist.ReduceOp.PRODUCT, dist.ReduceOp.MAX, dist.ReduceOp.MIN\n",
    "    # only rank 0 will have four\n",
    "    print(f\"[{rank}] data = {tensor[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/pvm/Desktop/nlp03/week6/distributed_training/Collective_Communication.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pvm/Desktop/nlp03/week6/distributed_training/Collective_Communication.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m do_reduce(rank \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, size \u001b[39m=\u001b[39;49m \u001b[39m4\u001b[39;49m )\n",
      "\u001b[1;32m/Users/pvm/Desktop/nlp03/week6/distributed_training/Collective_Communication.ipynb Cell 4\u001b[0m in \u001b[0;36mdo_reduce\u001b[0;34m(rank, size)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pvm/Desktop/nlp03/week6/distributed_training/Collective_Communication.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_reduce\u001b[39m(rank: \u001b[39mint\u001b[39m, size: \u001b[39mint\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pvm/Desktop/nlp03/week6/distributed_training/Collective_Communication.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# create a group with all processors\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pvm/Desktop/nlp03/week6/distributed_training/Collective_Communication.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     group \u001b[39m=\u001b[39m dist\u001b[39m.\u001b[39;49mnew_group(\u001b[39mlist\u001b[39;49m(\u001b[39mrange\u001b[39;49m(size)))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pvm/Desktop/nlp03/week6/distributed_training/Collective_Communication.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pvm/Desktop/nlp03/week6/distributed_training/Collective_Communication.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# sending all tensors to rank 0 and sum them\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/nlp03/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:3481\u001b[0m, in \u001b[0;36mnew_group\u001b[0;34m(ranks, timeout, backend, pg_options)\u001b[0m\n\u001b[1;32m   3421\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3422\u001b[0m \u001b[39mCreates a new distributed group.\u001b[39;00m\n\u001b[1;32m   3423\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3476\u001b[0m \u001b[39m    A handle of distributed group that can be given to collective calls.\u001b[39;00m\n\u001b[1;32m   3477\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3479\u001b[0m \u001b[39mglobal\u001b[39;00m _world\n\u001b[0;32m-> 3481\u001b[0m default_pg \u001b[39m=\u001b[39m _get_default_group()\n\u001b[1;32m   3482\u001b[0m default_backend, default_store \u001b[39m=\u001b[39m _world\u001b[39m.\u001b[39mpg_map[default_pg]\n\u001b[1;32m   3483\u001b[0m global_rank \u001b[39m=\u001b[39m default_pg\u001b[39m.\u001b[39mrank()\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/nlp03/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:707\u001b[0m, in \u001b[0;36m_get_default_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[39mGetting the default process group created by init_process_group\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_initialized():\n\u001b[0;32m--> 707\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    708\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDefault process group has not been initialized, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mplease make sure to call init_process_group.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    710\u001b[0m     )\n\u001b[1;32m    711\u001b[0m \u001b[39mreturn\u001b[39;00m GroupMember\u001b[39m.\u001b[39mWORLD\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "do_reduce(rank = 1, size = 4 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
